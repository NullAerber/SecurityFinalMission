\section{总结}
\subsection{基于二分类的机器学习}
逻辑回归判断和svm都是常见的机器学习分类算法，从目标函数来看，区别在于lg采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。两者的根本目的都是一样。因此在很多实验中，两种算法的结果是很接近的。

SVM的处理方法是只考虑支持向量，也就是和分类最相关的少数点去学习，对数据的异常点（离群值）比较敏感，而且输入svm的数据维度不宜太高，这就是为什么本文在第四部分中对数据进行聚类降维后SVM的效果会明显提升的原因。但是逻辑回归是通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。


逻辑回归相对来说模型更简单，更容易理解，实现起来，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，有一套结构化风险最小化的理论基础。
\subsection{基于LSTM序列模型分类器}
对于LSTM序列模型分类器来说，由于输入是json格式的数据，在对json格式的数据分类上还是比较优良的，在json模拟数据集上可以达到95\%左右的精确度，这说明这个模型确实有效。不过这些数据在模拟数据集（Mock API）上生成，生成的log数据可能有一定偏向性，测试数据集可能和训练数据集风格过于一致，所以精确度很高。以至于当使用普通的纯URL进行测试时，精度降了很多。可能也是因为样本已经完全不在一个分布，数据的格式也不太相同的原因。